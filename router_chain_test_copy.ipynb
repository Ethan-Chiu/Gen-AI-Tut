{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"]=\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environmental_template = \"\"\"You are a passionate environmentalist with a deep understanding of ecological systems and sustainability principles. \\\n",
    "You excel at providing insightful explanations and solutions related to environmental issues in a clear and engaging manner. \\\n",
    "When faced with a question outside your expertise, you gracefully acknowledge your limitations.\\\n",
    "\n",
    "Answer the following question based on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Here is the question:\n",
    "{input}\"\"\"\n",
    "\n",
    "education_template = \"\"\"You are an enthusiastic and knowledgeable educator with a profound understanding of pedagogy and learning principles. \\\n",
    "You are adept at delivering comprehensive explanations and fostering engaging learning experiences. \n",
    "\n",
    "Answer the following question with the provided context.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Here is the question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "{\n",
    "    \"name\": \"environmentalist\",\n",
    "    \"description\": \"Ideal for addressing environmental concerns and offering insightful solutions\",\n",
    "    \"prompt_template\": environmental_template,\n",
    "},\n",
    "{\n",
    "    \"name\": \"education\", \n",
    "    \"description\": \"Good for answering questions related to education or study or 早自習\", \n",
    "    \"prompt_template\": education_template,\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"\n",
    "你是一個router，你的工作是分析input的問題與甚麼領域有關，\\\n",
    "當問題不清楚時適當修改問題，接著把問題傳遞給適當的language model.\\\n",
    "你會得到所有可以選擇的language model名稱及它們擅長哪個領域的問題。\\\n",
    "\n",
    "You can also use the history messages provided here to make your decision.\n",
    "<< HISTORY >>\n",
    "{{history}}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "\n",
    "\\```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ summary of the history relevant to the topic of input\n",
    "}}}}\n",
    "\\```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is completely\\\n",
    "unrelated to any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" should contain a concise summary of the history to provide context for the input.\n",
    "\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\miniconda3\\envs\\genai-course\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-pro\", convert_system_message_to_human=True)\n",
    "claude = ChatAnthropic(temperature=0,  model_name=\"claude-3-haiku-20240307\")\n",
    "# openai = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document loader & text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.web_base import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector = Chroma.from_documents(documents, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=environmental_template)\n",
    "\n",
    "document_chain = create_stuff_documents_chain(claude, prompt)\n",
    "retriever = vector.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "enviromental_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=education_template)\n",
    "\n",
    "document_chain = create_stuff_documents_chain(claude, prompt)\n",
    "retriever = vector.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "education_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "general_chain = RunnablePassthrough.assign(\n",
    "    answer=(ChatPromptTemplate.from_template(\"{history} {input}\") | claude | StrOutputParser())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Route to Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    print(info)\n",
    "    if info[\"destination\"]:\n",
    "        if \"environmentalist\" in info[\"destination\"].lower():\n",
    "            return enviromental_chain\n",
    "        elif \"education\" in info[\"destination\"].lower():\n",
    "            return education_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "# destinations\n",
    "# ['physics: Good for answering questions about physics',\n",
    "#  'math: Good for answering math questions']\n",
    "\n",
    "router_system_template = PromptTemplate.from_template(MULTI_PROMPT_ROUTER_TEMPLATE)\n",
    "router_system_template = router_system_template.format(destinations=destinations_str)\n",
    "\n",
    "router_system_template = HumanMessagePromptTemplate.from_template(router_system_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "memory = ConversationBufferMemory(llm=gemini, max_token_limit=1024, ai_prefix=\"\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # MessagesPlaceholder(variable_name=\"history\", optional=True), # Put history into router llm ???\n",
    "        router_system_template,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.chains.router.llm_router import RouterOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "router_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | router_prompt\n",
    "    | claude\n",
    "    | RouterOutputParser()\n",
    "    | {\"destination\": itemgetter(\"destination\"), \"next_inputs\": lambda x: x[\"next_inputs\"]['input']} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multi_prompt_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        route=router_chain\n",
    "    )\n",
    "    | { \n",
    "        \"destination\": lambda x: x[\"route\"][\"destination\"], \n",
    "        \"input\": lambda x: f'{x[\"route\"][\"next_inputs\"]}\\n{x[\"input\"]}'\n",
    "    } \n",
    "    | RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    ) \n",
    "    | RunnableLambda(route)\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.server import Server\n",
    "from api.manager import Manager\n",
    "from api.models.match import Match\n",
    "\n",
    "server = Server(\"http://localhost:3000\", \"ca8bf4a0-ee0e-4c4b-be6d-5dd8fd6ae24e\")\n",
    "manager = Manager(server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'destination': None, 'input': \"The input 'test' is too short and vague to determine the appropriate language model. More context is needed to understand the user's intent.\\ntest\", 'history': ''}\n",
      "You're ready\n"
     ]
    }
   ],
   "source": [
    "from api.game import Game, validate_chain, show_result\n",
    "\n",
    "valid = validate_chain(multi_prompt_chain,memory)\n",
    "\n",
    "if valid:\n",
    "  print(\"You're ready\")\n",
    "  game = Game(server, multi_prompt_chain, memory)\n",
    "else:\n",
    "  print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You joined a match\n",
      "All matches:\n",
      "Match ID: 15, Name: Match Fri Jun 14 2024 00:01:00 GMT+0800 (Taipei Standard Time), Topic ID: 3, Match Status: GRADED, Players: [], History Messages: []\n",
      "Match ID: 16, Name: Match Fri Jun 14 2024 00:18:48 GMT+0800 (Taipei Standard Time), Topic ID: 2, Match Status: GRADED, Players: [], History Messages: []\n",
      "Match ID: 17, Name: Match Fri Jun 14 2024 00:21:44 GMT+0800 (Taipei Standard Time), Topic ID: 1, Match Status: GRADED, Players: [], History Messages: []\n",
      "Match ID: 18, Name: Match Fri Jun 14 2024 00:26:39 GMT+0800 (Taipei Standard Time), Topic ID: 2, Match Status: START, Players: [], History Messages: []\n",
      "Enter match: (ID: 18)\n",
      "Match ID: 18, Name: 2, Topic ID: Match Fri Jun 14 2024 00:26:39 GMT+0800 (Taipei Standard Time), Players: [<api.models.player.Player object at 0x000001557F77D930>, <api.models.player.Player object at 0x000001556B403F70>], Topic: Topic ID: 2, Description: This is a debate about ..., Is First: False, Result: None\n",
      "Topic: This is a debate about ...\n",
      "Match: 2\n",
      "Opponent's turn first\n",
      "Opponent ID: 1\n",
      "Opponent respond 0:00:00 seconds ago\n",
      "正方立場:\n",
      "\n",
      "我們支持披薩店推出創新的草仔粿烏龜披薩,因為這有助於滿足消費者多元化的需求。\n",
      "\n",
      "首先,草仔粿是台灣傳統的特色甜點,融入披薩中可以增加本土特色,吸引更多消費者嘗試。根據調查,有高達60%的台灣消費者對於結合台灣在地元素的新食品感興趣。\n",
      "\n",
      "其次,烏龜是許多人喜愛的可愛動物形象,將其融入披薩設計,不僅能引起消費者的好奇心,也能增加披薩的趣味性和話題性。這有助於提高披薩店的曝光度和銷量。\n",
      "\n",
      "再者,草仔粿烏龜披薩是一個嶄新的創意,能夠滿足消費者尋求新鮮有趣體驗的需求。根據業界研究,有超過75%的消費者表示願意嘗試創新的披薩口味。\n",
      "\n",
      "因此,我們認為披薩店推出這款草仔粿烏龜披薩是一個明智的決策,不僅能滿足消費者需求,也能為披薩店帶來更多商機。\n",
      "Opponent respond 0:00:05.357042 seconds ago\n",
      "current_input {'input': '作為反方，請堅守反對推出草仔粿披薩的立場，並簡明扼要地陳述您的意見和理由。\\n反方:'}\n",
      "{'destination': None, 'input': \"The discussion is about the reasonableness and business impact of Pizza Hut's new 'grass jelly turtle pizza' product. The pro side has argued that the innovative product can meet diverse consumer demands, increase brand exposure, and satisfy the need for novel experiences. Now the con side is asked to present their opposing views.\\n作為反方，請堅守反對推出草仔粿披薩的立場，並簡明扼要地陳述您的意見和理由。\\n反方:\", 'history': 'Human: Pizza Hut近期推出了口味極端的草仔粿烏龜披薩。我們現在請正反雙方展開辯論，探討這款披薩的合理性以及對披薩店業務的影響。\\n您能提出具體的統計、數據或相關陳述，或是談論目前事件的發展，將有助於深化討論並增加論點的說服力。\\n正方：支持草仔粿烏龜披薩\\n反方：反對草仔粿烏龜披薩\\n\\n作為正方，請堅守支持推出草仔粿披薩的立場，並簡明扼要地陳述您的意見和理由。\\n正方:\\n: 正方立場:\\n\\n我們支持披薩店推出創新的草仔粿烏龜披薩,因為這有助於滿足消費者多元化的需求。\\n\\n首先,草仔粿是台灣傳統的特色甜點,融入披薩中可以增加本土特色,吸引更多消費者嘗試。根據調查,有高達60%的台灣消費者對於結合台灣在地元素的新食品感興趣。\\n\\n其次,烏龜是許多人喜愛的可愛動物形象,將其融入披薩設計,不僅能引起消費者的好奇心,也能增加披薩的趣味性和話題性。這有助於提高披薩店的曝光度和銷量。\\n\\n再者,草仔粿烏龜披薩是一個嶄新的創意,能夠滿足消費者尋求新鮮有趣體驗的需求。根據業界研究,有超過75%的消費者表示願意嘗試創新的披薩口味。\\n\\n因此,我們認為披薩店推出這款草仔粿烏龜披薩是一個明智的決策,不僅能滿足消費者需求,也能為披薩店帶來更多商機。'}\n",
      "反方立場:\n",
      "\n",
      "我們反對披薩店推出草仔粿烏龜披薩,因為這款產品存在諸多問題,可能對披薩店的業務造成負面影響。\n",
      "\n",
      "首先,草仔粿和烏龜這兩種食材與傳統披薩的口味和質地差異太大,很難與披薩的主要成分如麵皮、起司和醬料融合在一起,可能會造成口感不協調和違和感。根據消費者反饋,有高達80%的人表示不會購買這種口味的披薩。\n",
      "\n",
      "其次,這種極端創新的產品可能會讓消費者感到困惑和不信任。披薩店應該專注於提供符合消費者期望的經典披薩口味,而不是過度追求新奇。過去有類似嘗試的披薩店,往往難以獲得消費者的認同。\n",
      "\n",
      "再者,推出這種特殊口味的披薩需要額外的成本投入,包括研發、生產和營銷。但考慮到銷量可能不佳,這種投資可能難以收回,反而會拖累披薩店的整體經營。\n",
      "\n",
      "因此,我們認為披薩店不應該推出這款草仔粿烏龜披薩,而是應該專注於提供符合消費者期望的經典披薩產品,以確保業務的穩定和健康發展。\n",
      "Message sent successfully.\n",
      "Opponent respond 0:00:25.716773 seconds ago\n",
      "作為正方,我們理解反方提出的一些擔憂,但我們仍然堅持支持披薩店推出這款創新的草仔粿烏龜披薩。\n",
      "\n",
      "首先,關於口感不協調的問題,我們認為披薩店可以通過精心的配方設計,將草仔粿和烏龜與披薩的主要成分巧妙融合,創造出獨特而協調的口感。事實上,根據我們的試吃反饋,有超過60%的消費者表示這款披薩的口味組合令人驚喜。\n",
      "\n",
      "其次,我們認為適度的創新不會讓消費者感到困惑和不信任,反而能夠吸引他們的好奇心。披薩店可以通過有效的市場推廣,向消費者傳達這款披薩的特色和價值,增加他們的接受度。事實上,有超過70%的消費者表示願意嘗試這種創新的披薩口味。\n",
      "\n",
      "最後,我們認為適度的成本投入是值得的。這款創新產品不僅能夠提高披薩店的曝光度和話題性,吸引新客群,還可能帶來長期的商業機會。根據我們的市場分析,如果這款披薩獲得消費者的廣泛認可,其銷量和利潤增長的潛力是巨大的。\n",
      "\n",
      "因此,我們堅信披薩店推出草仔粿烏龜披薩是一個明智的決策,不僅能滿足消費者的需求,也能為披薩店帶來新的商機。我們希望反方能夠重新考慮,支持這一創新嘗試。\n",
      "Opponent respond 0:00:05.052026 seconds ago\n",
      "current_input {'input': '作為反方，請堅守反對推出草仔粿披薩的立場，並針對正方所發表的意見進行反駁。\\n反方:'}\n",
      "{'destination': None, 'input': \"The discussion is about the reasonableness and business impact of Pizza Hut's new 'grass jelly turtle pizza' product. The pro side has argued that the innovative product can meet diverse consumer demands, increase brand exposure, and bring new business opportunities. The con side has raised concerns about the product's taste coordination, consumer confusion, and potential financial risks.\\n作為反方，請堅守反對推出草仔粿披薩的立場，並針對正方所發表的意見進行反駁。\\n反方:\", 'history': 'Human: Pizza Hut近期推出了口味極端的草仔粿烏龜披薩。我們現在請正反雙方展開辯論，探討這款披薩的合理性以及對披薩店業務的影響。\\n您能提出具體的統計、數據或相關陳述，或是談論目前事件的發展，將有助於深化討論並增加論點的說服力。\\n正方：支持草仔粿烏龜披薩\\n反方：反對草仔粿烏龜披薩\\n\\n作為正方，請堅守支持推出草仔粿披薩的立場，並簡明扼要地陳述您的意見和理由。\\n正方:\\n: 正方立場:\\n\\n我們支持披薩店推出創新的草仔粿烏龜披薩,因為這有助於滿足消費者多元化的需求。\\n\\n首先,草仔粿是台灣傳統的特色甜點,融入披薩中可以增加本土特色,吸引更多消費者嘗試。根據調查,有高達60%的台灣消費者對於結合台灣在地元素的新食品感興趣。\\n\\n其次,烏龜是許多人喜愛的可愛動物形象,將其融入披薩設計,不僅能引起消費者的好奇心,也能增加披薩的趣味性和話題性。這有助於提高披薩店的曝光度和銷量。\\n\\n再者,草仔粿烏龜披薩是一個嶄新的創意,能夠滿足消費者尋求新鮮有趣體驗的需求。根據業界研究,有超過75%的消費者表示願意嘗試創新的披薩口味。\\n\\n因此,我們認為披薩店推出這款草仔粿烏龜披薩是一個明智的決策,不僅能滿足消費者需求,也能為披薩店帶來更多商機。\\nHuman: 作為反方，請堅守反對推出草仔粿披薩的立場，並簡明扼要地陳述您的意見和理由。\\n反方:\\n: 反方立場:\\n\\n我們反對披薩店推出草仔粿烏龜披薩,因為這款產品存在諸多問題,可能對披薩店的業務造成負面影響。\\n\\n首先,草仔粿和烏龜這兩種食材與傳統披薩的口味和質地差異太大,很難與披薩的主要成分如麵皮、起司和醬料融合在一起,可能會造成口感不協調和違和感。根據消費者反饋,有高達80%的人表示不會購買這種口味的披薩。\\n\\n其次,這種極端創新的產品可能會讓消費者感到困惑和不信任。披薩店應該專注於提供符合消費者期望的經典披薩口味,而不是過度追求新奇。過去有類似嘗試的披薩店,往往難以獲得消費者的認同。\\n\\n再者,推出這種特殊口味的披薩需要額外的成本投入,包括研發、生產和營銷。但考慮到銷量可能不佳,這種投資可能難以收回,反而會拖累披薩店的整體經營。\\n\\n因此,我們認為披薩店不應該推出這款草仔粿烏龜披薩,而是應該專注於提供符合消費者期望的經典披薩產品,以確保業務的穩定和健康發展。\\nHuman: 作為正方，請堅守支持推出草仔粿披薩的立場，並針對反對方所發表的意見進行反駁。\\n正方:\\n: 作為正方,我們理解反方提出的一些擔憂,但我們仍然堅持支持披薩店推出這款創新的草仔粿烏龜披薩。\\n\\n首先,關於口感不協調的問題,我們認為披薩店可以通過精心的配方設計,將草仔粿和烏龜與披薩的主要成分巧妙融合,創造出獨特而協調的口感。事實上,根據我們的試吃反饋,有超過60%的消費者表示這款披薩的口味組合令人驚喜。\\n\\n其次,我們認為適度的創新不會讓消費者感到困惑和不信任,反而能夠吸引他們的好奇心。披薩店可以通過有效的市場推廣,向消費者傳達這款披薩的特色和價值,增加他們的接受度。事實上,有超過70%的消費者表示願意嘗試這種創新的披薩口味。\\n\\n最後,我們認為適度的成本投入是值得的。這款創新產品不僅能夠提高披薩店的曝光度和話題性,吸引新客群,還可能帶來長期的商業機會。根據我們的市場分析,如果這款披薩獲得消費者的廣泛認可,其銷量和利潤增長的潛力是巨大的。\\n\\n因此,我們堅信披薩店推出草仔粿烏龜披薩是一個明智的決策,不僅能滿足消費者的需求,也能為披薩店帶來新的商機。我們希望反方能夠重新考慮,支持這一創新嘗試。'}\n",
      "作為反方,我們仍然堅持反對披薩店推出這款草仔粿烏龜披薩,儘管正方提出了一些有趣的論點。\n",
      "\n",
      "首先,正方提到可以通過精心的配方設計來解決口感不協調的問題,但我們認為這仍然是一個巨大的挑戰。草仔粿和烏龜這些非典型的披薩配料,很難與傳統的麵皮、起司和醬料完美融合,即使經過精心設計,最終的口感也很難令人滿意。根據我們的市場調研,即使有60%的消費者表示驚喜,但仍有40%的人對口感表示不滿。這樣的分裂反應可能會影響整體銷量。\n",
      "\n",
      "其次,正方提到適度的創新不會讓消費者感到困惑,但我們認為這種極端的創新可能會超出大多數消費者的接受範圍。即使披薩店進行有效的市場推廣,也很難完全消除消費者的疑慮和不信任感。根據我們的調查,有超過70%的消費者表示雖然願意嘗試,但對這款披薩的整體印象並不太好。這可能會影響消費者的重複購買意願。\n",
      "\n",
      "最後,正方提到適度的成本投入是值得的,但我們認為這種創新產品的風險太高。即使能夠提高曝光度和吸引新客群,但如果最終銷量不佳,那麼這些投資可能難以收回。根據我們的財務分析,即使在最樂觀的情況下,這款披薩的利潤增長也很有限。相比之下,披薩店應該將資源集中在提供更受歡迎的經典披薩上,以確保穩定的業務發展。\n",
      "\n",
      "因此,我們仍然堅持反對披薩店推出這款草仔粿烏龜披薩,認為這將對披薩店的整體業務造成負面影響。我們希望披薩店能夠認真考慮我們的擔憂,專注於提供符合消費者期望的經典產品。\n",
      "Message sent successfully.\n",
      "Match over\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    game.join_match()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    game.cancel_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "\n",
    "inputs.append({\"input\": \n",
    "\"\"\"兩位參賽者進行辯論比賽，今天的討論議題是支不支持廢除早自習?\n",
    "正方：支持廢除早自習\n",
    "反方：反對廢除早自習\n",
    "\n",
    "作為正方，請堅守支持廢除早自習的立場，並簡明扼要地陳述正方的意見和理由。\n",
    "正方:\"\"\"})\n",
    "\n",
    "inputs.append({\"input\": \n",
    "\"\"\"作為反方，請堅守反對廢除早自習的立場，簡明扼要地陳述你的意見和理由。\n",
    "反方:\"\"\"\n",
    "})\n",
    "\n",
    "inputs.append({\"input\": \n",
    "\"\"\"作為正方，請堅守支持廢除早自習的立場。請針對反對方所發表的意見，進行反駁。\n",
    "正方:\"\"\"})\n",
    "\n",
    "inputs.append({\"input\": \n",
    "\"\"\"作為反方，請堅守反對廢除早自習的立場。請針對正方所發表的意見，進行反駁。\n",
    "反方:\"\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \n",
    "\"\"\"兩位參賽者進行辯論比賽，今天的討論議題是支不支持廢除早自習?\n",
    "正方：支持廢除早自習\n",
    "反方：反對廢除早自習\n",
    "\n",
    "作為正方，請堅守支持廢除早自習的立場，並簡明扼要地陳述正方的意見和理由。\n",
    "正方:\"\"\"}\n",
    "response = multi_prompt_chain.invoke(inputs)\n",
    "print(response.get(\"answer\"))\n",
    "memory.save_context(inputs, {\"output\": response.get(\"answer\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \n",
    "\"\"\"作為反方，請堅守反對廢除早自習的立場，簡明扼要地陳述你的意見和理由。\n",
    "反方:\"\"\"\n",
    "}\n",
    "response = multi_prompt_chain.invoke(inputs)\n",
    "print(response.get(\"answer\"))\n",
    "memory.save_context(inputs, {\"output\": response.get(\"answer\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \n",
    "\"\"\"作為正方，請堅守支持廢除早自習的立場。請針對反對方所發表的意見，進行反駁。\n",
    "正方:\"\"\"}\n",
    "response = multi_prompt_chain.invoke(inputs)\n",
    "print(response.get(\"answer\"))\n",
    "memory.save_context(inputs, {\"output\": response.get(\"answer\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \n",
    "\"\"\"作為反方，請堅守反對廢除早自習的立場。請針對正方所發表的意見，進行反駁。\n",
    "反方:\"\"\"}\n",
    "response = multi_prompt_chain.invoke(inputs)\n",
    "print(response.get(\"answer\"))\n",
    "memory.save_context(inputs, {\"output\": response.get(\"answer\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_prompt_chain.get_graph().print_ascii()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
