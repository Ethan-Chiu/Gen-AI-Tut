{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"]=\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environmental_template = \"\"\"You are a passionate environmentalist with a deep understanding of ecological systems and sustainability principles. \\\n",
    "You excel at providing insightful explanations and solutions related to environmental issues in a clear and engaging manner. \\\n",
    "When faced with a question outside your expertise, you gracefully acknowledge your limitations.\\\n",
    "\n",
    "Answer the following question based on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Here is the question:\n",
    "{input}\"\"\"\n",
    "\n",
    "education_template = \"\"\"You are an enthusiastic and knowledgeable educator with a profound understanding of pedagogy and learning principles. \\\n",
    "You are adept at delivering comprehensive explanations and fostering engaging learning experiences. \n",
    "\n",
    "Answer the following question with the provided context.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Here is the question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "{\n",
    "    \"name\": \"environmentalist\",\n",
    "    \"description\": \"Ideal for addressing environmental concerns and offering insightful solutions\",\n",
    "    \"prompt_template\": environmental_template,\n",
    "},\n",
    "{\n",
    "    \"name\": \"education\", \n",
    "    \"description\": \"Good for answering questions related to education or study or 早自習\", \n",
    "    \"prompt_template\": education_template,\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"\n",
    "你是一個router，你的工作是分析input的問題與甚麼領域有關，\\\n",
    "當問題不清楚時適當修改問題，接著把問題傳遞給適當的language model.\\\n",
    "你會得到所有可以選擇的language model名稱及它們擅長哪個領域的問題。\\\n",
    "\n",
    "You can also use the history messages provided here to make your decision.\n",
    "<< HISTORY >>\n",
    "{{history}}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "\n",
    "\\```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ summary of the history relevant to the topic of input\n",
    "}}}}\n",
    "\\```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is completely\\\n",
    "unrelated to any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" should contain a concise summary of the history to provide context for the input.\n",
    "\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-pro\", convert_system_message_to_human=True)\n",
    "claude = ChatAnthropic(temperature=0,  model_name=\"claude-3-haiku-20240307\")\n",
    "# openai = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document loader & text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.web_base import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector = Chroma.from_documents(documents, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=environmental_template)\n",
    "\n",
    "document_chain = create_stuff_documents_chain(claude, prompt)\n",
    "retriever = vector.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "enviromental_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=education_template)\n",
    "\n",
    "document_chain = create_stuff_documents_chain(claude, prompt)\n",
    "retriever = vector.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "education_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "general_chain = RunnablePassthrough.assign(\n",
    "    answer=(ChatPromptTemplate.from_template(\"{history} {input}\") | claude | StrOutputParser())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Route to Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    print(info)\n",
    "    if info[\"destination\"]:\n",
    "        if \"environmentalist\" in info[\"destination\"].lower():\n",
    "            return enviromental_chain\n",
    "        elif \"education\" in info[\"destination\"].lower():\n",
    "            return education_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "# destinations\n",
    "# ['physics: Good for answering questions about physics',\n",
    "#  'math: Good for answering math questions']\n",
    "\n",
    "router_system_template = PromptTemplate.from_template(MULTI_PROMPT_ROUTER_TEMPLATE)\n",
    "router_system_template = router_system_template.format(destinations=destinations_str)\n",
    "\n",
    "router_system_template = HumanMessagePromptTemplate.from_template(router_system_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "memory = ConversationBufferMemory(llm=gemini, max_token_limit=1024, ai_prefix=\"\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # MessagesPlaceholder(variable_name=\"history\", optional=True), # Put history into router llm ???\n",
    "        router_system_template,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.chains.router.llm_router import RouterOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "router_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | router_prompt\n",
    "    | claude\n",
    "    | RouterOutputParser()\n",
    "    | {\"destination\": itemgetter(\"destination\"), \"next_inputs\": lambda x: x[\"next_inputs\"]['input']} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multi_prompt_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        route=router_chain\n",
    "    )\n",
    "    | { \n",
    "        \"destination\": lambda x: x[\"route\"][\"destination\"], \n",
    "        \"input\": lambda x: f'{x[\"route\"][\"next_inputs\"]}\\n{x[\"input\"]}'\n",
    "    } \n",
    "    | RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    ) \n",
    "    | RunnableLambda(route)\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.server import Server\n",
    "from api.manager import Manager\n",
    "from api.models.match import Match\n",
    "\n",
    "server = Server(\"http://localhost:3000\", \"d8f8bc5e-4176-468c-8d78-5fdd73a50fe5\")\n",
    "manager = Manager(server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.game import Game, validate_chain, show_result\n",
    "\n",
    "valid = validate_chain(multi_prompt_chain,memory)\n",
    "\n",
    "if valid:\n",
    "  print(\"You're ready\")\n",
    "  game = Game(server, multi_prompt_chain, memory)\n",
    "else:\n",
    "  print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    game.join_match()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    game.cancel_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "\n",
    "inputs.append({\"input\": \n",
    "\"\"\"兩位參賽者進行辯論比賽，今天的討論議題是支不支持廢除早自習?\n",
    "正方：支持廢除早自習\n",
    "反方：反對廢除早自習\n",
    "\n",
    "作為正方，請堅守支持廢除早自習的立場，並簡明扼要地陳述正方的意見和理由。\n",
    "正方:\"\"\"})\n",
    "\n",
    "inputs.append({\"input\": \n",
    "\"\"\"作為反方，請堅守反對廢除早自習的立場，簡明扼要地陳述你的意見和理由。\n",
    "反方:\"\"\"\n",
    "})\n",
    "\n",
    "inputs.append({\"input\": \n",
    "\"\"\"作為正方，請堅守支持廢除早自習的立場。請針對反對方所發表的意見，進行反駁。\n",
    "正方:\"\"\"})\n",
    "\n",
    "inputs.append({\"input\": \n",
    "\"\"\"作為反方，請堅守反對廢除早自習的立場。請針對正方所發表的意見，進行反駁。\n",
    "反方:\"\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \n",
    "\"\"\"兩位參賽者進行辯論比賽，今天的討論議題是支不支持廢除早自習?\n",
    "正方：支持廢除早自習\n",
    "反方：反對廢除早自習\n",
    "\n",
    "作為正方，請堅守支持廢除早自習的立場，並簡明扼要地陳述正方的意見和理由。\n",
    "正方:\"\"\"}\n",
    "response = multi_prompt_chain.invoke(inputs)\n",
    "print(response.get(\"answer\"))\n",
    "memory.save_context(inputs, {\"output\": response.get(\"answer\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \n",
    "\"\"\"作為反方，請堅守反對廢除早自習的立場，簡明扼要地陳述你的意見和理由。\n",
    "反方:\"\"\"\n",
    "}\n",
    "response = multi_prompt_chain.invoke(inputs)\n",
    "print(response.get(\"answer\"))\n",
    "memory.save_context(inputs, {\"output\": response.get(\"answer\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \n",
    "\"\"\"作為正方，請堅守支持廢除早自習的立場。請針對反對方所發表的意見，進行反駁。\n",
    "正方:\"\"\"}\n",
    "response = multi_prompt_chain.invoke(inputs)\n",
    "print(response.get(\"answer\"))\n",
    "memory.save_context(inputs, {\"output\": response.get(\"answer\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"input\": \n",
    "\"\"\"作為反方，請堅守反對廢除早自習的立場。請針對正方所發表的意見，進行反駁。\n",
    "反方:\"\"\"}\n",
    "response = multi_prompt_chain.invoke(inputs)\n",
    "print(response.get(\"answer\"))\n",
    "memory.save_context(inputs, {\"output\": response.get(\"answer\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_prompt_chain.get_graph().print_ascii()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
